{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# <u>Section Neural Network</u>\n",
    "Here is the Neural Network part.\n",
    "\n",
    "## <u>Math and equations</u>\n",
    "\n",
    "Here you are going to find all the math needed to understand the code and equations. This part was written using the chapter 4 of the Fitzpatrick - Plasma Physic and Introduction that you can find in the bibliography\n",
    "### <u>Fluid dynamics</u>\n",
    "Let's start with the first equation derived from fluid dynamics.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{d f_s }{d t} + \\mathbf{v} + \\nabla f_s + \\mathbf{a_s} \\nabla_s f_s = C_s(f)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "\\nabla \\equiv \\frac{d}{d \\mathbf{r}}\n",
    "\\quad and \\quad\n",
    "\\nabla_s \\equiv \\frac{d}{d \\mathbf{v}}\n",
    "\\quad and \\quad\n",
    "a_s = \\frac{e_s}{m_s}(\\mathbf{E} + \\mathbf{v} * \\mathbf{B})\n",
    "$$\n",
    "\n",
    "$e_s \\, and \\, m_s $ are special electic charges and masses and $\\mathbf{E} \\, and \\,  \\mathbf{B}$ are the ensemble-average electronagmetic fields.\n",
    "\n",
    "Never the less, this expression is extremly deficult to compute due to the intrasec collision of the plasma. If we assume a collision free plasma, which we can do due to the low amount of collisions on the top of the plasma of the sun, we can then transform the fluid equation with a simplified version of the kinetic equation and then get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{d f_s }{d t} + \\mathbf{v} + \\nabla f_s + \\mathbf{a_s} \\nabla_s f_s = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is the Vaslov Equation. Continuing in this project, we will be working with this equation.\n",
    "\n",
    "\n",
    "### <u>Moment of Distribution Function</u>\n",
    "\n",
    "The Kth velocity space moment is extremly important to understand and easily get some important physical propreties and values. We can start with the first equation, the Kth velocity space moment of the ensemble average distribution function $f_s(\\mathbf{r},\\mathbf{v},t)$ is written\n",
    "\n",
    "$$\n",
    "\\mathbf{M}_k(\\mathbf{r},t) = \\int \\mathbf{v}\\mathbf{v}...\\mathbf{v} f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "\n",
    "with k factors of $\\mathbf{v}$ and $\\mathbf{M}_k$ a tensor for rank k.  \n",
    "If $f_s$ is suffenciently smooth, $\\mathbf{M}_k$ can be viewed as a displaced Gaussian distribution function  specified by three moments: M0, the vector M1, and the scalar formed by contracting M2.\n",
    "\n",
    "These low order moments all have simplified physical interpretation.  \n",
    "For the first order, we get the Particule Number Density (PND):\n",
    "$$\n",
    "n_s(\\mathbf{r},t) = \\int f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "\n",
    "and the Particule Flux Density (PFD):\n",
    "$$\n",
    "n_s(\\mathbf{r},t) \\mathbf{V}_s(\\mathbf{r},t) = \\int \\mathbf{v} f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "with $\\mathbf{V}_s$ the flow velocity.  \n",
    "If PND and PFD are respectively summed up with regards to there electic charge and/or speed, we can find the Charge Density and the Charge Flux:\n",
    "$$\n",
    "\\rho = \\sum_s e_s n_s\n",
    "\\quad and \\quad\n",
    "\\mathbf{j} = \\sum_s e_s n_s \\mathbf{V}_s\n",
    "$$\n",
    "\n",
    "For the second order moment, we get a value that describes the flow momentum in the laboratory frame called the Stress Tensor (ST):\n",
    "$$\n",
    "\\mathbf{P}_s(\\mathbf{r},t) = \\int m_s \\mathbf{v}\\mathbf{v} f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "\n",
    "And for the Third and last order, the Energy Flux Density (EFD) is obtained:\n",
    "$$\n",
    "\\mathbf{Q}_s(\\mathbf{r},t) = \\int \\frac{1}{2} m_s v^2 \\mathbf{v} f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "  \n",
    "  \n",
    "II is quiet difficult to measure the second and third order moment in a dynamical frame but easier in the rest-frame. In that frame, those quantities will have different names: ST will be called pressure tensor $\\mathbf{p}_s$ and EFD will become the heat flux density $\\mathbf{q}_s$.  \n",
    "We one to the other with the relative velocity $\\mathbf{w}_s \\equiv \\mathbf{v} - \\mathbf{V}_s$ as:\n",
    "$$\n",
    "\\mathbf{p}_s(\\mathbf{r},t) = \\int m_s \\mathbf{w}_s\\mathbf{w}_s f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "\\quad and \\quad\n",
    "\\mathbf{q}_s(\\mathbf{r},t) = \\int \\frac{1}{2} m_s w_{s}^{2} \\mathbf{w}_s f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "With this pressor tensor, we can extract the real or scalar pressure:\n",
    "$$\n",
    "p_s \\equiv \\frac{1}{3} \\text{Tr}(\\mathbf{p}_s)\n",
    "\\quad \\iff \\quad\n",
    "\\frac{2}{3} p_s = \\int m_s w_{s}^{2} f_s(\\mathbf{r},\\mathbf{v},t)\\, d^3\\mathbf{v}\n",
    "$$\n",
    "Since we are at the rest-frame (Thermodynamic Equilibrium), the distribution function becomes a Maxwellian characterized by some temperature T. With $p=nT$, we get the kinetic temperature:\n",
    "$$T_s=\\frac{p_s}{n_s}$$\n",
    "\n",
    "We just need to relate the two frames togethere. To do that, we will use the relative velocity and by direct substitution, we get:\n",
    "$$\n",
    "\\mathbf{P}_s = \\mathbf{p}_s + m_s n_s \\mathbf{V}_s \\mathbf{V}_s\n",
    "\\quad and \\quad\n",
    "\\mathbf{Q}_s = \\mathbf{q}_s + \\mathbf{p}_s \\mathbf{V}_s + \\frac{2}{3} p_s \\mathbf{V}_s + \\frac{1}{2} m_s n_s V_{s}^{2} \\mathbf{V}_s\n",
    "$$"
   ],
   "id": "12ec1265524d6fcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## <u> Coding and Implementation Part</u>\n",
    "In this part, we are going to cover the coding part. Here is a lay out of the steps:  \n",
    "    - Import  \n",
    "    - Equations and Numerical Methodes  \n",
    "    - Neural Network"
   ],
   "id": "c871a2c0159303e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <u> Import</u>\n",
    "Here are all the packages used in the Project."
   ],
   "id": "4204611b184a738e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:30:17.550520Z",
     "start_time": "2024-10-16T11:30:14.130075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ],
   "id": "576c8576a631af51",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <u> Equations and Numerical Methodes</u>\n",
    "Here are all the Equations formulated in python as well as the numerical methodes used."
   ],
   "id": "d926275504042bcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:30:17.582429Z",
     "start_time": "2024-10-16T11:30:17.573433Z"
    }
   },
   "cell_type": "code",
   "source": "a=5",
   "id": "6c3a18c925424218",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <u> Neural Network</u>\n",
    "Here are the implementations of the math equations in pythorch using NN to solve the issue"
   ],
   "id": "104ba5754222ce45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section we:\n",
    "- Define Hyperparameters\n",
    "- Indicate what data to use  \n",
    "- Define NeuralNetwork() Class + iteration  \n",
    "- Define train_loop Function\n",
    "- Define test_loop Function"
   ],
   "id": "3741534e19fdab4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T11:30:23.848354Z",
     "start_time": "2024-10-16T11:30:17.589429Z"
    }
   },
   "source": [
    "#Mettre la data ici et faire en sorte que ca colle bien en temps que Data set\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "#Ici on créer notre Neural Network. Ce qui est très important de bien définir les couches que tu veux et combien. Aussi dans quel ordre. \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "#Ici on itère notre NeuralNetwork\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "#The train_loop is the training loop. It will do the front and backwards propagation and update the weights.\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "#Unlike the train_loop, this loop is here to see if we make progress in finding the write answers or not.\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "2.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we are going to Run our NN",
   "id": "19faa6faa16d1bf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:32:36.671091Z",
     "start_time": "2024-10-16T11:30:24.552682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Hyperparameters: Parameters that will define how our NN learn and adapt\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "id": "eb1676903e4aacdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295261  [   64/60000]\n",
      "loss: 2.287113  [ 6464/60000]\n",
      "loss: 2.267157  [12864/60000]\n",
      "loss: 2.259916  [19264/60000]\n",
      "loss: 2.241296  [25664/60000]\n",
      "loss: 2.216079  [32064/60000]\n",
      "loss: 2.215578  [38464/60000]\n",
      "loss: 2.192358  [44864/60000]\n",
      "loss: 2.188671  [51264/60000]\n",
      "loss: 2.151575  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.146805 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.161028  [   64/60000]\n",
      "loss: 2.153364  [ 6464/60000]\n",
      "loss: 2.096512  [12864/60000]\n",
      "loss: 2.108312  [19264/60000]\n",
      "loss: 2.060270  [25664/60000]\n",
      "loss: 2.000411  [32064/60000]\n",
      "loss: 2.020244  [38464/60000]\n",
      "loss: 1.954831  [44864/60000]\n",
      "loss: 1.959572  [51264/60000]\n",
      "loss: 1.877385  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 1.883207 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.921637  [   64/60000]\n",
      "loss: 1.894292  [ 6464/60000]\n",
      "loss: 1.780676  [12864/60000]\n",
      "loss: 1.813069  [19264/60000]\n",
      "loss: 1.708438  [25664/60000]\n",
      "loss: 1.657738  [32064/60000]\n",
      "loss: 1.669559  [38464/60000]\n",
      "loss: 1.587455  [44864/60000]\n",
      "loss: 1.611086  [51264/60000]\n",
      "loss: 1.493426  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 1.520880 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.593940  [   64/60000]\n",
      "loss: 1.557841  [ 6464/60000]\n",
      "loss: 1.411073  [12864/60000]\n",
      "loss: 1.477080  [19264/60000]\n",
      "loss: 1.363935  [25664/60000]\n",
      "loss: 1.356174  [32064/60000]\n",
      "loss: 1.361976  [38464/60000]\n",
      "loss: 1.301413  [44864/60000]\n",
      "loss: 1.330313  [51264/60000]\n",
      "loss: 1.225891  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.256550 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.338074  [   64/60000]\n",
      "loss: 1.317605  [ 6464/60000]\n",
      "loss: 1.152360  [12864/60000]\n",
      "loss: 1.258289  [19264/60000]\n",
      "loss: 1.135686  [25664/60000]\n",
      "loss: 1.159902  [32064/60000]\n",
      "loss: 1.174639  [38464/60000]\n",
      "loss: 1.123379  [44864/60000]\n",
      "loss: 1.153284  [51264/60000]\n",
      "loss: 1.071523  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.092322 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.166696  [   64/60000]\n",
      "loss: 1.167022  [ 6464/60000]\n",
      "loss: 0.982208  [12864/60000]\n",
      "loss: 1.121994  [19264/60000]\n",
      "loss: 0.992913  [25664/60000]\n",
      "loss: 1.026470  [32064/60000]\n",
      "loss: 1.057957  [38464/60000]\n",
      "loss: 1.008944  [44864/60000]\n",
      "loss: 1.037632  [51264/60000]\n",
      "loss: 0.974443  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.985421 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.046724  [   64/60000]\n",
      "loss: 1.069486  [ 6464/60000]\n",
      "loss: 0.865053  [12864/60000]\n",
      "loss: 1.030334  [19264/60000]\n",
      "loss: 0.901560  [25664/60000]\n",
      "loss: 0.931253  [32064/60000]\n",
      "loss: 0.980666  [38464/60000]\n",
      "loss: 0.933571  [44864/60000]\n",
      "loss: 0.957124  [51264/60000]\n",
      "loss: 0.908868  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.912026 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.957662  [   64/60000]\n",
      "loss: 1.001322  [ 6464/60000]\n",
      "loss: 0.780803  [12864/60000]\n",
      "loss: 0.965254  [19264/60000]\n",
      "loss: 0.840289  [25664/60000]\n",
      "loss: 0.860716  [32064/60000]\n",
      "loss: 0.925873  [38464/60000]\n",
      "loss: 0.882679  [44864/60000]\n",
      "loss: 0.899190  [51264/60000]\n",
      "loss: 0.861606  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.859151 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.888983  [   64/60000]\n",
      "loss: 0.950031  [ 6464/60000]\n",
      "loss: 0.717952  [12864/60000]\n",
      "loss: 0.916606  [19264/60000]\n",
      "loss: 0.796847  [25664/60000]\n",
      "loss: 0.807207  [32064/60000]\n",
      "loss: 0.884204  [38464/60000]\n",
      "loss: 0.846867  [44864/60000]\n",
      "loss: 0.856338  [51264/60000]\n",
      "loss: 0.825508  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.819337 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.834402  [   64/60000]\n",
      "loss: 0.909178  [ 6464/60000]\n",
      "loss: 0.669418  [12864/60000]\n",
      "loss: 0.879030  [19264/60000]\n",
      "loss: 0.764315  [25664/60000]\n",
      "loss: 0.766200  [32064/60000]\n",
      "loss: 0.850450  [38464/60000]\n",
      "loss: 0.820399  [44864/60000]\n",
      "loss: 0.823832  [51264/60000]\n",
      "loss: 0.796621  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.787980 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
